---
- name: Hosted-Engine final tasks
  block:
  - name: Choose IPv4, IPv6 or auto
    import_tasks: ipv_switch.yml
  - name: Trigger hosted engine OVF update and enable the serial console
    ovirt_vm:
      id: "{{ he_vm_details.vm.id }}"
      description: "Hosted engine VM"
      serial_console: true
      auth: "{{ ovirt_auth }}"
  - name: Wait until OVF update finishes
    ovirt_storage_domain_info:
      auth: "{{ ovirt_auth }}"
      fetch_nested: true
      nested_attributes:
        - name
        - image_id
        - id
      pattern: "name={{ he_storage_domain_name }}"
    retries: 12
    delay: 10
    register: storage_domain_details
    until: "storage_domain_details.ovirt_storage_domains[0].disks | selectattr('name', 'match', '^OVF_STORE$') | list"
  - debug: var=storage_domain_details
  - name: Parse OVF_STORE disk list
    set_fact:
      ovf_store_disks: >-
        {{ storage_domain_details.ovirt_storage_domains[0].disks |
        selectattr('name', 'match', '^OVF_STORE$') | list }}
  - debug: var=ovf_store_disks
  - name: Check OVF_STORE volume status
    command: >-
      vdsm-client Volume getInfo storagepoolID={{ datacenter_id }}
      storagedomainID={{ storage_domain_details.ovirt_storage_domains[0].id }}
      imageID={{ item.id }} volumeID={{ item.image_id  }}
    environment: "{{ he_cmd_lang }}"
    changed_when: true
    register: ovf_store_status
    retries: 12
    delay: 10
    until: >-
      ovf_store_status.rc == 0 and ovf_store_status.stdout|from_json|json_query('status') == 'OK' and
      ovf_store_status.stdout|from_json|json_query('description')|from_json|json_query('Updated')
    with_items: "{{ ovf_store_disks }}"
  - debug: var=ovf_store_status
  - name: Wait for OVF_STORE disk content
    shell: >-
      vdsm-client Image prepare storagepoolID={{ datacenter_id }}
      storagedomainID={{ storage_domain_details.ovirt_storage_domains[0].id }} imageID={{ item.id }}
      volumeID={{ item.image_id  }} | grep path | awk '{ print $2 }' |
      xargs -I{} sudo -u vdsm dd if={} | tar -tvf - {{ he_vm_details.vm.id }}.ovf
    environment: "{{ he_cmd_lang }}"
    changed_when: true
    register: ovf_store_content
    retries: 12
    delay: 10
    until: ovf_store_content.rc == 0
    with_items: "{{ ovf_store_disks }}"
    args:
      warn: false
  - name: Prepare images
    command: >-
      vdsm-client Image prepare storagepoolID={{ datacenter_id }}
      storagedomainID={{ storage_domain_details.ovirt_storage_domains[0].id }}
      imageID={{ item.disk.id }} volumeID={{ item.disk.image_id  }}
    environment: "{{ he_cmd_lang }}"
    with_items:
      - "{{ he_virtio_disk_details }}"
      - "{{ he_conf_disk_details }}"
      - "{{ he_metadata_disk_details }}"
      - "{{ he_sanlock_disk_details }}"
    register: prepareimage_results
    changed_when: true
  - debug: var=prepareimage_results
  - name: Fetch Hosted Engine configuration disk path
    set_fact:
      he_conf_disk_path: >-
        {{ (prepareimage_results.results|json_query("[?item.id=='" +
        he_conf_disk_details.id + "'].stdout")|first|from_json).path }}
  - name: Fetch Hosted Engine virtio disk path
    set_fact:
      he_virtio_disk_path: >-
        {{ (prepareimage_results.results|json_query("[?item.id=='" +
        he_virtio_disk_details.id + "'].stdout")|first|from_json).path }}
  - name: Fetch Hosted Engine virtio metadata path
    set_fact:
      he_metadata_disk_path: >-
        {{ (prepareimage_results.results|json_query("[?item.id=='" +
        he_metadata_disk_details.id + "'].stdout")|first|from_json).path }}
  - debug: var=he_conf_disk_path
  - debug: var=he_virtio_disk_path
  - debug: var=he_metadata_disk_path
  - name: Shutdown local VM
    command: "virsh -c qemu:///system?authfile={{ he_libvirt_authfile }} shutdown {{ he_vm_name }}Local"
    environment: "{{ he_cmd_lang }}"
  - name: Wait for local VM shutdown
    command: virsh -r domstate "{{ he_vm_name }}Local"
    environment: "{{ he_cmd_lang }}"
    changed_when: true
    register: dominfo_out
    until: dominfo_out.rc == 0 and 'shut off' in dominfo_out.stdout
    retries: 120
    delay: 5
  - debug: var=dominfo_out
  - name: Undefine local VM
    command: "virsh -c qemu:///system?authfile={{ he_libvirt_authfile }} undefine {{ he_vm_name }}Local"
    environment: "{{ he_cmd_lang }}"
  - name: Update libvirt default network configuration, destroy
    command: "virsh -c qemu:///system?authfile={{ he_libvirt_authfile }} net-destroy default"
    environment: "{{ he_cmd_lang }}"
  - name: Update libvirt default network configuration, undefine
    command: "virsh -c qemu:///system?authfile={{ he_libvirt_authfile }} net-undefine default"
    environment: "{{ he_cmd_lang }}"
    ignore_errors: true
  - name: Detect ovirt-hosted-engine-ha version
    command: >-
      {{ ansible_python.executable }} -c
      'from ovirt_hosted_engine_ha.agent import constants as agentconst; print(agentconst.PACKAGE_VERSION)'
    environment: "{{ he_cmd_lang }}"
    register: ha_version_out
    changed_when: true
  - name: Set ha_version
    set_fact: ha_version="{{ ha_version_out.stdout_lines|first }}"
  - debug: var=ha_version
  - name: Create configuration templates
    template:
      src: "{{ item.src }}"
      dest: "{{ item.dest }}"
      mode: 0644
    with_items:
      - {src: templates/vm.conf.j2, dest: "{{ he_local_vm_dir }}/vm.conf"}
      - {src: templates/broker.conf.j2, dest: "{{ he_local_vm_dir }}/broker.conf"}
      - {src: templates/version.j2, dest: "{{ he_local_vm_dir }}/version"}
      - {src: templates/fhanswers.conf.j2, dest: "{{ he_local_vm_dir }}/fhanswers.conf"}
      - {src: templates/hosted-engine.conf.j2, dest: "{{ he_local_vm_dir }}/hosted-engine.conf"}
  - name: Create configuration archive
    command: >-
      tar --record-size=20480 -cvf {{ he_conf_disk_details.disk.image_id }}
      vm.conf broker.conf version fhanswers.conf hosted-engine.conf
    environment: "{{ he_cmd_lang }}"
    args:
      chdir: "{{ he_local_vm_dir }}"
      warn: false
    changed_when: true
    tags: ['skip_ansible_lint']
  - name: Create ovirt-hosted-engine-ha run directory
    file:
      path: /var/run/ovirt-hosted-engine-ha
      state: directory
      mode: 0755
  - name: Copy configuration files to the right location on host
    copy:
      remote_src: true
      src: "{{ item.src }}"
      dest: "{{ item.dest }}"
      mode: 0644
    with_items:
      - {src: "{{ he_local_vm_dir }}/vm.conf", dest: /var/run/ovirt-hosted-engine-ha}
      - {src: "{{ he_local_vm_dir }}/hosted-engine.conf", dest: /etc/ovirt-hosted-engine/}
  - name: Copy configuration archive to storage
    command: >-
      dd bs=20480 count=1 oflag=direct if="{{ he_local_vm_dir }}/{{ he_conf_disk_details.disk.image_id }}"
      of="{{ he_conf_disk_path }}"
    environment: "{{ he_cmd_lang }}"
    become: true
    become_user: vdsm
    become_method: sudo
    changed_when: true
    args:
      warn: false
  - name: Initialize metadata volume
    command: dd bs=1M count=1024 oflag=direct if=/dev/zero of="{{ he_metadata_disk_path }}"
    environment: "{{ he_cmd_lang }}"
    become: true
    become_user: vdsm
    become_method: sudo
    changed_when: true
  - include_tasks: get_local_vm_disk_path.yml
  - name: Generate DHCP network configuration for the engine VM
    template:
      src: templates/ifcfg-eth0-dhcp.j2
      dest: "{{ he_local_vm_dir }}/ifcfg-eth0"
      owner: root
      group: root
      mode: 0644
    when: he_vm_ip_addr is none
  - name: Generate static network configuration for the engine VM, IPv4
    template:
      src: templates/ifcfg-eth0-static.j2
      dest: "{{ he_local_vm_dir }}/ifcfg-eth0"
      owner: root
      group: root
      mode: 0644
    when: he_vm_ip_addr is not none and he_vm_ip_addr | ipv4
  - name: Generate static network configuration for the engine VM, IPv6
    template:
      src: templates/ifcfg-eth0-static-ipv6.j2
      dest: "{{ he_local_vm_dir }}/ifcfg-eth0"
      owner: root
      group: root
      mode: 0644
    when: he_vm_ip_addr is not none and he_vm_ip_addr | ipv6
  - name: Inject network configuration with guestfish
    command: >-
      guestfish -a {{ local_vm_disk_path }} --rw -i copy-in "{{ he_local_vm_dir }}/ifcfg-eth0"
      /etc/sysconfig/network-scripts {{ ":" }} selinux-relabel /etc/selinux/targeted/contexts/files/file_contexts
      /etc/sysconfig/network-scripts/ifcfg-eth0 force{{ ":" }}true
    environment:
      LIBGUESTFS_BACKEND: direct
      LANG: en_US.UTF-8
      LC_MESSAGES: en_US.UTF-8
      LC_ALL: en_US.UTF-8
    changed_when: true
  - name: Extract /etc/hosts from the Hosted Engine VM
    command: virt-copy-out -a {{ local_vm_disk_path }} /etc/hosts "{{ he_local_vm_dir }}"
    environment:
      LIBGUESTFS_BACKEND: direct
      LANG: en_US.UTF-8
      LC_MESSAGES: en_US.UTF-8
      LC_ALL: en_US.UTF-8
    changed_when: true
  - name: Clean /etc/hosts for the Hosted Engine VM for Engine VM FQDN
    lineinfile:
      dest: "{{ he_local_vm_dir }}/hosts"
      regexp: "# hosted-engine-setup-{{ hostvars[he_ansible_host_name]['he_local_vm_dir'] }}$"
      state: absent
  - name: Add an entry on /etc/hosts for the Hosted Engine VM for the VM itself
    lineinfile:
      dest: "{{ he_local_vm_dir }}/hosts"
      line: "{{ he_vm_ip_addr }} {{ he_fqdn }}"
      state: present
    when: he_vm_etc_hosts and he_vm_ip_addr is not none
  - name: Clean /etc/hosts for the Hosted Engine VM for host address
    lineinfile:
      dest: "{{ he_local_vm_dir }}/hosts"
      line: "{{ he_host_ip }} {{ he_host_address }}"
      state: absent
    when: not he_vm_etc_hosts
  - name: Inject /etc/hosts with guestfish
    command: >-
      guestfish -a {{ local_vm_disk_path }} --rw -i copy-in "{{ he_local_vm_dir }}/hosts"
      /etc {{ ":" }} selinux-relabel /etc/selinux/targeted/contexts/files/file_contexts
      /etc/hosts force{{ ":" }}true
    environment:
      LIBGUESTFS_BACKEND: direct
      LANG: en_US.UTF-8
      LC_MESSAGES: en_US.UTF-8
      LC_ALL: en_US.UTF-8
    changed_when: true
  - name: Copy local VM disk to shared storage
    command: >-
      qemu-img convert -f qcow2 -O raw -t none -T none {{ local_vm_disk_path }} {{ he_virtio_disk_path }}
    environment: "{{ he_cmd_lang }}"
    become: true
    become_user: vdsm
    become_method: sudo
    changed_when: true
  - name: Verify copy of VM disk
    command: qemu-img compare {{ local_vm_disk_path }} {{ he_virtio_disk_path }}
    environment: "{{ he_cmd_lang }}"
    become: true
    become_user: vdsm
    become_method: sudo
    changed_when: true
    when: he_debug_mode|bool
  - name: Remove temporary entry in /etc/hosts for the local VM
    lineinfile:
      dest: /etc/hosts
      regexp: "# temporary entry added by hosted-engine-setup for the bootstrap VM$"
      state: absent
  - name: Start ovirt-ha-broker service on the host
    service:
      name: ovirt-ha-broker
      state: started
      enabled: true
  - name: Initialize lockspace volume
    command: hosted-engine --reinitialize-lockspace --force
    environment: "{{ he_cmd_lang }}"
    register: result
    until: result.rc == 0
    ignore_errors: true
    retries: 5
    delay: 10
    changed_when: true
  - debug: var=result
  - block:
      - name: Workaround for ovirt-ha-broker start failures
        # Ugly workaround for https://bugzilla.redhat.com/1768511
        # fix it on ovirt-ha-broker side and remove ASAP
        systemd:
          state: restarted
          enabled: true
          name: ovirt-ha-broker
      - name: Initialize lockspace volume
        command: hosted-engine --reinitialize-lockspace --force
        environment: "{{ he_cmd_lang }}"
        register: result2
        until: result2.rc == 0
        retries: 5
        delay: 10
        changed_when: true
      - debug: var=result2
    when: result.rc != 0
  - name: Start ovirt-ha-agent service on the host
    service:
      name: ovirt-ha-agent
      state: started
      enabled: true
  - name: Exit HE maintenance mode
    command: hosted-engine --set-maintenance --mode=none
    environment: "{{ he_cmd_lang }}"
    register: mresult
    until: mresult.rc == 0
    retries: 3
    delay: 10
    changed_when: true
  - debug: var=mresult
  - name: Wait for the engine to come up on the target VM
    block:
      - name: Check engine VM health
        command: hosted-engine --vm-status --json
        environment: "{{ he_cmd_lang }}"
        register: health_result
        until: >-
          health_result.rc == 0 and 'health' in health_result.stdout and
          health_result.stdout|from_json|json_query('*."engine-status"."health"')|first=="good"
        retries: 180
        delay: 5
        changed_when: true
      - debug: var=health_result
    rescue:
      - name: Check VM status at virt level
        shell: virsh -r list | grep {{ he_vm_name }} | grep running
        environment: "{{ he_cmd_lang }}"
        ignore_errors: true
        changed_when: true
        register: vm_status_virsh
      - debug: var=vm_status_virsh
      - name: Fail if engine VM is not running
        fail:
          msg: "Engine VM is not running, please check vdsm logs"
        when: vm_status_virsh.rc != 0
      - name: Get target engine VM IP address
        shell: getent {{ ip_key }} {{ he_fqdn }} | cut -d' ' -f1 | uniq
        environment: "{{ he_cmd_lang }}"
        register: engine_vm_ip
        changed_when: true
      - name: Get VDSM's target engine VM stats
        command: vdsm-client VM getStats vmID={{ he_vm_details.vm.id }}
        environment: "{{ he_cmd_lang }}"
        register: engine_vdsm_stats
        changed_when: true
      - name: Convert stats to JSON format
        set_fact: json_stats={{ engine_vdsm_stats.stdout|from_json }}
      - name: Get target engine VM IP address from VDSM stats
        set_fact: engine_vm_ip_vdsm={{ json_stats[0].guestIPs }}
      - debug: var=engine_vm_ip_vdsm
      - name: Fail if Engine IP is different from engine's he_fqdn resolved IP
        fail:
          msg: >-
            Engine VM IP address is {{ engine_vm_ip_vdsm }} while the engine's he_fqdn {{ he_fqdn }} resolves to
            {{ engine_vm_ip.stdout_lines[0] }}. If you are using DHCP, check your DHCP reservation configuration
        when: engine_vm_ip_vdsm != engine_vm_ip.stdout_lines[0]
      - name: Fail is for any other reason the engine didn't started
        fail:
          msg: The engine failed to start inside the engine VM; please check engine.log.
  - name: Get target engine VM address
    shell: getent {{ ip_key }} {{ he_fqdn }} | cut -d ' ' -f1 | uniq
    environment: "{{ he_cmd_lang }}"
    register: engine_vm_ip
    when: engine_vm_ip is not defined
    changed_when: true
  # Workaround for ovn-central being configured with the address of the bootstrap VM.
  # Keep this aligned with:
  # https://github.com/oVirt/ovirt-engine/blob/master/packaging/playbooks/roles/ovirt-provider-ovn-driver/tasks/main.yml
  - name: Reconfigure OVN central address
    command: vdsm-tool ovn-config {{ engine_vm_ip.stdout_lines[0] }} {{ he_mgmt_network }}
    environment: "{{ he_cmd_lang }}"
    changed_when: true
  # Workaround for https://bugzilla.redhat.com/1540107
  # the engine fails deleting a VM if its status in the engine DB
  # is not up to date.
  - include_tasks: auth_sso.yml
  - name: Check for the local bootstrap VM
    ovirt_vm_info:
      pattern: id="{{ external_local_vm_uuid.stdout_lines|first }}"
      auth: "{{ ovirt_auth }}"
    register: local_vm_f
  - name: Remove the bootstrap local VM
    block:
      - name: Make the engine aware that the external VM is stopped
        ignore_errors: true
        ovirt_vm:
          state: stopped
          id: "{{ external_local_vm_uuid.stdout_lines|first }}"
          auth: "{{ ovirt_auth }}"
        register: vmstop_result
      - debug: var=vmstop_result
      - name: Wait for the local bootstrap VM to be down at engine eyes
        ovirt_vm_info:
          pattern: id="{{ external_local_vm_uuid.stdout_lines|first }}"
          auth: "{{ ovirt_auth }}"
        register: local_vm_status
        until: local_vm_status.ovirt_vms[0].status == "down"
        retries: 24
        delay: 5
      - debug: var=local_vm_status
      - name: Remove bootstrap external VM from the engine
        ovirt_vm:
          state: absent
          id: "{{ external_local_vm_uuid.stdout_lines|first }}"
          auth: "{{ ovirt_auth }}"
        register: vmremove_result
      - debug: var=vmremove_result
    when: local_vm_f.ovirt_vms|length > 0
  - name: Remove ovirt-engine-appliance rpm
    yum:
      name: ovirt-engine-appliance
      state: absent
    register: yum_result
    until: yum_result is success
    retries: 10
    delay: 5
    when: he_remove_appliance_rpm|bool

  - name: Include custom tasks for after setup customization
    include_tasks: "{{ item }}"
    with_fileglob: "hooks/after_setup/*.yml"
    register: after_setup_results
  - debug: var=after_setup_results
